{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fbe9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class AirbnbPreprocessorAndTrainer:\n",
    "    def __init__(self, csv_path, nrows=1000, image_size=32, batch_size=64, lr=0.001, patience=5, seed=42):\n",
    "        self.csv_path = csv_path\n",
    "        self.nrows = nrows\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.patience = patience\n",
    "\n",
    "        # Reproducibility\n",
    "        self.seed = seed\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Verwende Gerät:\", self.device)\n",
    "\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.model = None\n",
    "\n",
    "        # Diese werden dynamisch gesetzt\n",
    "        self.feature_columns = None  # Wird in preprocess() gesetzt\n",
    "        \n",
    "        self.image_column = \"picture_url\"\n",
    "        self.target_column = \"price\"\n",
    "        self.extra_columns = [\"id\"]\n",
    "\n",
    "        self.train_loader = None\n",
    "        self.test_loader = None\n",
    "        self.train_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.df = None\n",
    "        self.images = None\n",
    "\n",
    "    def preprocess(self):\n",
    "        df = pd.read_csv(self.csv_path, nrows=self.nrows)\n",
    "        \n",
    "        # Dynamische Feature-Auswahl: alle Spalten außer Ziel-, Bild- und ID-Spalte\n",
    "        self.feature_columns = [col for col in df.columns \n",
    "                               if col not in [self.image_column, self.target_column, \"id\"]]\n",
    "        \n",
    "        # print(f\"Verwende {len(self.feature_columns)} Features: {self.feature_columns[:10]}...\")  # Zeige erste 10\n",
    "        \n",
    "        needed = self.feature_columns + [self.image_column, self.target_column] + self.extra_columns\n",
    "        df = df[[col for col in needed if col in df.columns]].copy()\n",
    "        \n",
    "        # Drop rows ohne Preis oder Bild\n",
    "        df = df.dropna(subset=[self.target_column, self.image_column])\n",
    "        \n",
    "        # Preis bereinigen\n",
    "        df[self.target_column] = df[self.target_column].astype(str).str.replace(\"[$,]\", \"\", regex=True)\n",
    "        df[self.target_column] = pd.to_numeric(df[self.target_column], errors=\"coerce\")\n",
    "        df = df[df[self.target_column].notna()]\n",
    "        df[self.target_column] = np.log(df[self.target_column])\n",
    "        \n",
    "        # Index nach allen dropna-Operationen zurücksetzen\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        # Spalten nach Typ kategorisieren\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "        \n",
    "        # Entferne Ziel-, Bild- und ID-Spalten aus Features\n",
    "        numeric_cols = [col for col in numeric_cols \n",
    "                       if col not in [self.target_column, \"id\"]]\n",
    "        categorical_cols = [col for col in categorical_cols \n",
    "                           if col not in [self.image_column, \"id\"]]\n",
    "        \n",
    "        print(f\"Numerische Spalten ({len(numeric_cols)}): {numeric_cols[:5]}...\")\n",
    "        print(f\"Kategorische Spalten ({len(categorical_cols)}): {categorical_cols[:5]}...\")\n",
    "        \n",
    "        # Kategorische Variablen verarbeiten\n",
    "        encoded_dfs = []\n",
    "        for col in categorical_cols:\n",
    "            if col in df.columns:\n",
    "                # Boolean-Variablen\n",
    "                unique_vals = set(df[col].dropna().unique())\n",
    "                if df[col].dtype == 'bool' or unique_vals.issubset({'t', 'f', True, False}):\n",
    "                    df[col] = df[col].map({'t': 1, 'f': 0, True: 1, False: 0}).fillna(0).astype(float)\n",
    "                else:\n",
    "                    # One-Hot Encoding für andere kategorische Variablen\n",
    "                    # Nur wenn nicht zu viele unique values\n",
    "                    if df[col].nunique() < 50:  # Maximal 50 kategorien\n",
    "                        encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "                        encoded = encoder.fit_transform(df[[col]].fillna('missing'))\n",
    "                        encoded_cols = [f\"{col}_{str(cat).replace(' ', '_')}\" for cat in encoder.categories_[0]]\n",
    "                        encoded_df = pd.DataFrame(encoded, columns=encoded_cols, index=df.index)\n",
    "                        encoded_dfs.append(encoded_df)\n",
    "                        df = df.drop(col, axis=1)\n",
    "                    else:\n",
    "                        # Zu viele Kategorien - entfernen\n",
    "                        # print(f\"Spalte {col} hat {df[col].nunique()} Kategorien - wird entfernt\")\n",
    "                        df = df.drop(col, axis=1)\n",
    "        \n",
    "        # Encoded Features hinzufügen\n",
    "        if encoded_dfs:\n",
    "            df = pd.concat([df] + encoded_dfs, axis=1)\n",
    "        \n",
    "        # Index nach concat wieder zurücksetzen\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        # Numerische Imputation - KORRIGIERTE VERSION\n",
    "        numeric_cols_existing = [col for col in numeric_cols if col in df.columns]\n",
    "        \n",
    "        # Filter für Spalten, die nicht nur NaN sind\n",
    "        valid_numeric_cols = []\n",
    "        for col in numeric_cols_existing:\n",
    "            if df[col].notna().any():  # Hat mindestens einen nicht-NaN Wert\n",
    "                valid_numeric_cols.append(col)\n",
    "            else:\n",
    "                # print(f\"Spalte {col} enthält nur fehlende Werte und wird entfernt\")\n",
    "                df = df.drop(col, axis=1)  # Entferne Spalte komplett\n",
    "        \n",
    "        if len(valid_numeric_cols) > 0:\n",
    "            # Erstelle neuen Imputer für den aktuellen DataFrame\n",
    "            current_imputer = SimpleImputer(strategy='median')\n",
    "            numeric_data = df[valid_numeric_cols].copy()\n",
    "            \n",
    "            if numeric_data.shape[0] > 0:\n",
    "                # Imputation direkt auf dem aktuellen DataFrame\n",
    "                imputed_values = current_imputer.fit_transform(numeric_data)\n",
    "                \n",
    "                # WICHTIG: Verwende nur die Spalten, die tatsächlich im Output sind\n",
    "                output_cols = current_imputer.get_feature_names_out(valid_numeric_cols)\n",
    "                \n",
    "                # Zuweisung: nur für Spalten, die tatsächlich imputed wurden\n",
    "                for i, col in enumerate(valid_numeric_cols):\n",
    "                    if i < imputed_values.shape[1]:  # Sicherheitscheck\n",
    "                        df[col] = imputed_values[:, i]\n",
    "        \n",
    "        # Alle Feature-Spalten skalieren (außer Ziel-, Bild- und ID-Spalten)\n",
    "        scale_cols = [col for col in df.columns \n",
    "                      if col not in [self.image_column, self.target_column, \"id\"]]\n",
    "        \n",
    "        if scale_cols and len(scale_cols) > 0:\n",
    "            # Erstelle neuen Scaler für den aktuellen DataFrame\n",
    "            current_scaler = MinMaxScaler()\n",
    "            scale_data = df[scale_cols].copy()\n",
    "            \n",
    "            if scale_data.shape[0] > 0:\n",
    "                scaled_values = current_scaler.fit_transform(scale_data)\n",
    "                \n",
    "                # Zuweisung: gleiche Anzahl Zeilen \n",
    "                for i, col in enumerate(scale_cols):\n",
    "                    if i < scaled_values.shape[1]:  # Sicherheitscheck\n",
    "                        df[col] = scaled_values[:, i]\n",
    "        \n",
    "        # Finale Bereinigung: alle NaN entfernen\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        self.df = df\n",
    "        print(f\"Finale Datenform: {df.shape}\")\n",
    "        print(f\"Feature-Spalten: {len(scale_cols)}\")\n",
    "\n",
    "    def process_images(self):\n",
    "        images = []\n",
    "        valid_indices = []\n",
    "\n",
    "        for i, row in tqdm(self.df.iterrows(), total=len(self.df), desc=\"Bilder verarbeiten\"):\n",
    "            try:\n",
    "                url = row[self.image_column]\n",
    "                response = requests.get(url, timeout=5)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                content_type = response.headers.get('Content-Type', '')\n",
    "                if 'image' not in content_type:\n",
    "                    raise ValueError(f\"Kein Bild-Content (Type: {content_type})\")\n",
    "\n",
    "                img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "                img = img.resize((self.image_size, self.image_size))\n",
    "                images.append(np.array(img))\n",
    "                valid_indices.append(i)\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler bei Index {i}: {e}\")\n",
    "\n",
    "        self.images = np.array(images)\n",
    "        self.df = self.df.iloc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "    def prepare_tensors(self):\n",
    "        feature_cols = [col for col in self.df.columns if col not in [self.image_column, self.target_column, \"id\"]]\n",
    "        X_tab = self.df[feature_cols].values.astype(np.float32)\n",
    "        y = self.df[self.target_column].values.astype(np.float32)\n",
    "\n",
    "        # Train-Test-Split (tabular, images, target)\n",
    "        (X_train_tab, X_test_tab,\n",
    "         X_train_img, X_test_img,\n",
    "         y_train, y_test) = train_test_split(\n",
    "            X_tab, self.images, y, test_size=0.2, random_state=self.seed\n",
    "        )\n",
    "\n",
    "        # Datasets\n",
    "        self.train_dataset = self.AirbnbDataset(X_train_img, X_train_tab, y_train)\n",
    "        self.test_dataset = self.AirbnbDataset(X_test_img, X_test_tab, y_test)\n",
    "\n",
    "        # DataLoader\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    class AirbnbDataset(Dataset):\n",
    "        def __init__(self, images, tab_features, prices, transform=None):\n",
    "            self.images = images\n",
    "            self.tab_features = tab_features\n",
    "            self.prices = prices\n",
    "            self.transform = transform or transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.images)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img = self.images[idx]\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            tab_data = torch.tensor(self.tab_features[idx], dtype=torch.float32)\n",
    "            price = torch.tensor([self.prices[idx]], dtype=torch.float32)\n",
    "            return (img, tab_data), price\n",
    "\n",
    "    class MultiInputPricePredictor(nn.Module):\n",
    "        def __init__(self, tab_dim):\n",
    "            super().__init__()\n",
    "            # Bild-Zweig (CNN)\n",
    "            self.image_branch = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),      # 16x16\n",
    "                nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),      # 8x8\n",
    "                nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),      # 4x4\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            \n",
    "            # Erweiterte tabellarische Zweig für viele Features\n",
    "            self.tab_branch = nn.Sequential(\n",
    "                nn.Linear(tab_dim, 128),  # Größere erste Schicht\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(128, 64),       # Weitere Schicht\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(64, 32)         # Finale Feature-Dimension\n",
    "            )\n",
    "            \n",
    "            # Gemeinsamer Regressor\n",
    "            self.regressor = nn.Sequential(\n",
    "                nn.Linear(64 * 4 * 4 + 32, 256),  # Größere kombinierte Schicht\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(256, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            img, tab = x\n",
    "            img_features = self.image_branch(img)\n",
    "            tab_features = self.tab_branch(tab)\n",
    "            combined = torch.cat((img_features, tab_features), dim=1)\n",
    "            return self.regressor(combined)\n",
    "\n",
    "    def train_model(self, epochs=50):\n",
    "        tab_dim = self.train_dataset.tab_features.shape[1]\n",
    "        model = self.MultiInputPricePredictor(tab_dim).to(self.device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            total_train_loss = 0.0\n",
    "            for (images, tab_data), prices in self.train_loader:\n",
    "                images = images.to(self.device)\n",
    "                tab_data = tab_data.to(self.device)\n",
    "                prices = prices.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model((images, tab_data))\n",
    "                loss = criterion(outputs, prices)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item() * images.size(0)\n",
    "\n",
    "            epoch_train_loss = total_train_loss / len(self.train_loader.dataset)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            total_val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for (images, tab_data), prices in self.test_loader:\n",
    "                    images = images.to(self.device)\n",
    "                    tab_data = tab_data.to(self.device)\n",
    "                    prices = prices.to(self.device)\n",
    "\n",
    "                    outputs = model((images, tab_data))\n",
    "                    loss = criterion(outputs, prices)\n",
    "                    total_val_loss += loss.item() * images.size(0)\n",
    "\n",
    "            epoch_val_loss = total_val_loss / len(self.test_loader.dataset)\n",
    "            val_losses.append(epoch_val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "            # Early Stopping\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "                print(\"Neues bestes Modell gespeichert\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= self.patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "        self.model = model\n",
    "        return train_losses, val_losses\n",
    "\n",
    "    def calculate_rmse(self, loader):\n",
    "        \"\"\"\n",
    "        RMSE auf der Original-Preisskala .\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        all_preds, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for (images, tab_data), prices in loader:\n",
    "                images = images.to(self.device)\n",
    "                tab_data = tab_data.to(self.device)\n",
    "                prices = prices.to(self.device)\n",
    "\n",
    "                outputs = self.model((images, tab_data))\n",
    "                all_preds.extend(outputs.cpu().numpy().flatten())\n",
    "                all_targets.extend(prices.cpu().numpy().flatten())\n",
    "\n",
    "        # zurücktransformieren von log -> price\n",
    "        preds_price = np.exp(np.array(all_preds))\n",
    "        targets_price = np.exp(np.array(all_targets))\n",
    "\n",
    "        rmse = np.sqrt(np.mean((preds_price - targets_price) ** 2))\n",
    "        return rmse\n",
    "\n",
    "\n",
    "    def regression_metrics_real(self, preds_log, targets_log):\n",
    "        \"\"\"\n",
    "        Regression-Metriken auf echter Preis-Skala (nicht log).\n",
    "        \"\"\"\n",
    "        preds_price = np.exp(preds_log)\n",
    "        targets_price = np.exp(targets_log)\n",
    "\n",
    "        mae = mean_absolute_error(targets_price, preds_price)\n",
    "        rmse = np.sqrt(np.mean((preds_price - targets_price) ** 2))\n",
    "        medae = median_absolute_error(targets_price, preds_price)\n",
    "        rme = np.mean((preds_price - targets_price) / targets_price) * 100  # % Error\n",
    "        r2 = r2_score(targets_price, preds_price)\n",
    "\n",
    "        return mae, rmse, medae, rme, r2\n",
    "\n",
    "\n",
    "    def evaluate_loader(self, loader, model, device):\n",
    "        preds, targets = [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for (imgs, tabs), prices in loader:\n",
    "                imgs = imgs.to(device)\n",
    "                tabs = tabs.to(device)\n",
    "                outputs = model((imgs, tabs))\n",
    "                preds.extend(outputs.cpu().numpy().flatten())\n",
    "                targets.extend(prices.cpu().numpy().flatten())\n",
    "        return np.array(preds), np.array(targets)\n",
    "\n",
    "\n",
    "# ------------------ Zusatz: Tabellenmetriken, Plots & Learning-Curve ------------------ #\n",
    "\n",
    "def build_results_row(trainer, split_name, loader):\n",
    "    preds_log, targets_log = trainer.evaluate_loader(loader, trainer.model, trainer.device)\n",
    "    mae, rmse, medae, rme, r2 = trainer.regression_metrics_real(preds_log, targets_log)\n",
    "    return {\n",
    "        \"Modell\": \"MLP+CNN\",\n",
    "        \"Split\": split_name,\n",
    "        \"RMSE ($)\": rmse,\n",
    "        \"R² ($)\": r2,\n",
    "        \"MAE ($)\": mae,\n",
    "        \"MedAE ($)\": medae\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_true_vs_pred_price(trainer, loader, title=\"True vs. Predicted Price – MLP+CNN\", max_price=500):\n",
    "    preds_log, targets_log = trainer.evaluate_loader(loader, trainer.model, trainer.device)\n",
    "    y_true_price = np.exp(targets_log)\n",
    "    preds_price = np.exp(preds_log)\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(y_true_price, preds_price, alpha=0.4)\n",
    "    plt.plot([0, max_price], [0, max_price], 'r--', label='Ideal')\n",
    "    plt.xlim(0, max_price)\n",
    "    plt.ylim(0, max_price)\n",
    "    plt.xlabel(\"True Price ($)\")\n",
    "    plt.ylabel(\"Predicted Price ($)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curve_pytorch(trainer,\n",
    "                                train_fracs=np.linspace(0.1, 1.0, 10),\n",
    "                                epochs_per_frac=6,\n",
    "                                use_log_rmse=True,\n",
    "                                shuffle=True):\n",
    "    # Volle Trainingsdaten aus dem vorhandenen Dataset\n",
    "    imgs_full = trainer.train_dataset.images\n",
    "    tabs_full = trainer.train_dataset.tab_features\n",
    "    y_full = trainer.train_dataset.prices\n",
    "    n = len(imgs_full)\n",
    "\n",
    "    idx_all = np.arange(n)\n",
    "    if shuffle:\n",
    "        rng = np.random.default_rng(trainer.seed)\n",
    "        rng.shuffle(idx_all)\n",
    "\n",
    "    def make_loader_from_indices(indices):\n",
    "        subset_ds = trainer.AirbnbDataset(imgs_full[indices], tabs_full[indices], y_full[indices])\n",
    "        return DataLoader(subset_ds, batch_size=trainer.batch_size, shuffle=True)\n",
    "\n",
    "    def rmse_on_loader(loader, model):\n",
    "        preds, targets = [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for (imgs_b, tabs_b), y_b in loader:\n",
    "                imgs_b = imgs_b.to(trainer.device)\n",
    "                tabs_b = tabs_b.to(trainer.device)\n",
    "                out = model((imgs_b, tabs_b)).cpu().numpy().flatten()\n",
    "                tg = y_b.cpu().numpy().flatten()\n",
    "                if not use_log_rmse:\n",
    "                    out = np.exp(out)\n",
    "                    tg = np.exp(tg)\n",
    "                preds.extend(out)\n",
    "                targets.extend(tg)\n",
    "        preds = np.array(preds)\n",
    "        targets = np.array(targets)\n",
    "        return np.sqrt(np.mean((preds - targets) ** 2))\n",
    "\n",
    "    train_sizes, train_rmse, val_rmse = [], [], []\n",
    "\n",
    "    for frac in train_fracs:\n",
    "        m = max(1, int(n * frac))\n",
    "        indices = idx_all[:m]\n",
    "        train_loader_frac = make_loader_from_indices(indices)\n",
    "\n",
    "        # Frisches Modell je Trainingsgröße\n",
    "        tab_dim = trainer.train_dataset.tab_features.shape[1]\n",
    "        model = trainer.MultiInputPricePredictor(tab_dim).to(trainer.device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=trainer.lr)\n",
    "\n",
    "        # Kurzes Training pro Größe\n",
    "        for _ in range(epochs_per_frac):\n",
    "            model.train()\n",
    "            for (imgs_b, tabs_b), y_b in train_loader_frac:\n",
    "                imgs_b = imgs_b.to(trainer.device)\n",
    "                tabs_b = tabs_b.to(trainer.device)\n",
    "                y_b = y_b.to(trainer.device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model((imgs_b, tabs_b))\n",
    "                loss = criterion(out, y_b)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        train_sizes.append(m)\n",
    "        train_rmse.append(rmse_on_loader(train_loader_frac, model))\n",
    "        val_rmse.append(rmse_on_loader(trainer.test_loader, model))\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(train_sizes, train_rmse, 'o-', label=f\"Training RMSE {'(log)' if use_log_rmse else '($)'}\")\n",
    "    plt.plot(train_sizes, val_rmse, 'o-', label=f\"Validation RMSE {'(log)' if use_log_rmse else '($)'}\")\n",
    "    plt.xlabel(\"Trainingsgröße\")\n",
    "    plt.ylabel(f\"RMSE {'(log)' if use_log_rmse else '($)'}\")\n",
    "    plt.title(\"Learning Curve – MLP+CNN\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Pfad anpassen\n",
    "    csv_path = \"data/listings.csv.gz\"\n",
    "\n",
    "    # Initialisieren und Daten verarbeiten\n",
    "    trainer = AirbnbPreprocessorAndTrainer(csv_path, nrows=10000, image_size=32, batch_size=64, lr=0.001, patience=5, seed=42)\n",
    "    trainer.preprocess()\n",
    "    trainer.process_images()\n",
    "    trainer.prepare_tensors()\n",
    "\n",
    "    # Trainieren\n",
    "    train_losses, val_losses = trainer.train_model(epochs=30)\n",
    "\n",
    "    # Bestes Modell laden\n",
    "    trainer.model.load_state_dict(torch.load('best_model.pth', map_location=trainer.device))\n",
    "\n",
    "   # Log-RMSE\n",
    "    test_rmse_log = trainer.calculate_rmse(trainer.test_loader)\n",
    "    print(f\"Test RMSE (log): {test_rmse_log:.4f}\")\n",
    "\n",
    "    # RMSE in $ auf echter Skala\n",
    "    test_rmse_real = trainer.calculate_rmse(trainer.test_loader)\n",
    "    print(f\"Test RMSE (real $): {test_rmse_real:.2f}\")\n",
    "\n",
    "\n",
    "    # Tabellenmetriken (nur log)\n",
    "    results = []\n",
    "    results.append(build_results_row(trainer, \"Test\", trainer.test_loader))\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(by=\"RMSE ($)\").reset_index(drop=True)\n",
    "    print(\"\\nErgebnistabelle (MLP+CNN):\")\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    # True-vs-Predicted-Plots\n",
    "    plot_true_vs_pred_price(trainer, trainer.test_loader, title=\"True vs. Predicted Price – MLP+CNN\", max_price=500)\n",
    "\n",
    "    # Learning-Curve für das PyTorch-Modell (in log scale)\n",
    "    plot_learning_curve_pytorch(trainer,\n",
    "                                train_fracs=np.linspace(0.1, 1.0, 10),\n",
    "                                epochs_per_frac=6,\n",
    "                                use_log_rmse=True,\n",
    "                                shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189214c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_losses, label=\"Train Loss (log)\")\n",
    "plt.plot(val_losses, label=\"Val Loss (log)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training/Validation Loss (log)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Methode 1: Direktes Lesen mit pandas\n",
    "df = pd.read_csv(\"data/listings.csv.gz\")\n",
    "\n",
    "# Methode 2: Mit gzip-Modul\n",
    "import gzip\n",
    "with gzip.open(\"data/listings.csv.gz\", \"rt\") as f:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "# Methode 3: Spezifizierte Komprimierung\n",
    "df = pd.read_csv(\"data/listings.csv.gz\", compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle Spaltennamen anzeigen\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# DataFrame-Info anzeigen\n",
    "print(df.info())\n",
    "\n",
    "# Erste paar Zeilen anzeigen\n",
    "print(df.head())\n",
    "\n",
    "# Shape der Daten\n",
    "print(f\"Anzahl Zeilen: {df.shape[0]}, Anzahl Spalten: {df.shape[1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
